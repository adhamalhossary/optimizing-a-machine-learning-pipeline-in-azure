# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This project uses data from direct marketing campaigns of a Portuguese banking institution. The marketing campaigns are based on phone calls. It contains 20 features such as age, job and martial status. The target column contain only two categories, Yes and No, to determine if the client subscribed to the bank's term deposit. 

The aim of the algorithms built using the Python SDK (w/ Hyperdrive) and AutoML is to accurately predict if a potential client will subscribe to the bank's term deposit or not. This is to assist them in targeting thier resources in approaching clients that are more likely to subscribe.

The best performing model was found using the AutoML run and was a Voting Ensemble with an accuracy of 91.78%. However, the Logistic classifier trained using Hyperdrive had an accuracy of 91.44% which is very close to the accuracy of the Voting Ensemble model.

## Scikit-learn and Hyperdrive Pipeline

### Scikit-learn

A Logistic Regression model was first created and trained using Scikit-learn in the train.py. The steps taken in the python script were as follows:

1 - Import the banking dataset using Azure TabularDataset Factory

2 - Data is then cleaned and transformed using a cleaning function

3 - Processed data is then split into a training and testing set

4 - Scikit-learn was used to train an initial Logistic Regression model while specificing the value of two hyper parameters, C and max_iter. C represents the inverse of the regularization strength, while max_iter represents the maximum number of interations taken for the model to converge. These two parameters were initially passed in the python script so they can be optimised later on using Hyperdrive.

5 - The trained model is then saved

The model had parameters of C = 0.1 and max_iter = 100, and achieved an accuracy of 91.43%
### Hyper Drive

The initial model trained is then optimised using Hyperdrive. Hyperdrive is a method of implementing automatic hyperparameter tuning. Hyperparameter tuning is typically computationally expensive and manual, therefore, by using Hyperdrive we are able to automate this process and run experiments in parallel to efficiently optimize hyperparameters.

The steps taken to implement Hyperdrive were as follows:

1 - Configuration of the Azure cloud resources
2 - Configuring the Hyperdrive
3 - Running the Hyperdrive
4 - Retrieving the model with the parameters that gave the best model

Elaborating more on the second step in configuring the hyper drive, there are two extremely beneficial parameters that are included in the configuration; RandomParameterSampling and BanditPolicy.

**RandomParameterSampling** is a parameter sampler that randomly selects hyperparameter values from a wide range specified by the user to train the model. This is much better than a grid sweep as it is not as computationally expensive and time-consuming, and can choose parameters that make the model achieve high accuracy. Random Sampler also supports early termination of low-performance runs, thus saving on computational resources. The parameters passed to the random sampler were:

- C: 0.01,0.1,10,100
- max_iter: 50,100,150,200

**BanditPolicy** is an early termination policy that terminates runs early if they are not achieving the same perfomance as the best model. This also adds to improving computational efficiency and saving time as it automatically terminates models with a poor perfomance.

The best model had parameters of C = 10 and max_iter = 50, and achieved an accuracy of 91.44%. Took 20mins

## AutoML

The steps taken to implement AutoML were as follows:

1 - Import the banking dataset using Azure TabularDataset Factory
2 - Data is then cleaned and transformed using the cleaning function in train.py
3 - AutoML was configured and a run was submitted to find the model with the best performace
4 - The best model was saved

The best performing model was a Voting Ensemble model with an accuracy of 91.78%. The hyper parameters of the model were as follows:

-
-
-

**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline Comparison
When comparing both pipelines together, AutoML seems to have the advantage due to:

- Less steps taken to find the best model (Simpler architecture)
- Achieved better accuracy
- Time and runs?

I think the main advantage of automl compared to hyperdrive is the ability of automl to test different algorithms easily. We might think that the model chosen was the best for this problem and try to optimize the hyperparameters using hyperdrive. However, there might be a model we didn't try that might performs better than the model we chose, which is what happened in our case. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

The main area of improvement is to take the voting ensemble algorithm from the automl run and tune the hyper parameters using hyper drive to achieve better accuracy. We could also take the top five performing models and try and optimize them as well.

We could also test the models' performances from the hyperdrive and automl runs against different error metrics such as the AUC.
